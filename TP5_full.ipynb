{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0c70d56",
   "metadata": {},
   "source": [
    "# Policy Gradients and REINFORCE\n",
    "\n",
    "\n",
    "Goal for the session:\n",
    "\n",
    "Use the setting of last week to study Policy Gradients\n",
    "\n",
    "HÃ©di Hadiji April 2023  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3192463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a202847",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea50e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python --version = 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]\n",
      "torch.__version__ = 2.2.2+cpu\n",
      "np.__version__ = 1.26.2\n",
      "gym.__version__ = 0.29.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"python --version = {sys.version}\")\n",
    "print(f\"torch.__version__ = {torch.__version__}\")\n",
    "print(f\"np.__version__ = {np.__version__}\")\n",
    "print(f\"gym.__version__ = {gym.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c38d6dd",
   "metadata": {},
   "source": [
    "# Setting up the agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66eaf259",
   "metadata": {},
   "source": [
    "### Baselines and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ac7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent: \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state, **kwargs):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self, *data):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72b72807",
   "metadata": {},
   "source": [
    "## Implementing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e15cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(agent, env, n_sim=10):\n",
    "    \"\"\"    \n",
    "    Monte Carlo evaluation of the agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the agent policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action = agent.get_action(state, epsilon=0)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5703dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "agent = RandomAgent(env.observation_space, env.action_space)\n",
    "\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, epsilon=0)\n",
    "        state, reward, done, _, _ = display_env.step(action)\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'Episode length {rewards}')\n",
    "    \n",
    "run_one_episode(env, agent, display=True)\n",
    "print(f'Average over 5 runs : {np.mean(eval_agent(agent, env))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2c766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, N_episodes, eval_every=100, reward_threshold=400, n_eval=10):\n",
    "    total_time = 0\n",
    "    for ep in range(N_episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_time += 1\n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            mean_reward = np.mean(eval_agent(agent, env, n_sim=n_eval))\n",
    "            print(\"episode =\", ep+1, \", reward = \", mean_reward)\n",
    "            if mean_reward >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76134c9",
   "metadata": {},
   "source": [
    "## Theory Recap\n",
    "\n",
    "$$\n",
    "    \\nabla J(\\theta) \\approx \\sum_{t=1}^T \\gamma^t G_t \\nabla \\log \\pi (A_t \\mid S_t, \\theta)\n",
    "$$\n",
    "where $\\approx$ means that we estimate the LHS by the RHS\n",
    "\n",
    "\n",
    "In this session, the policy is computed to be the softmax of the output of a neural network by \n",
    "\n",
    "$$\n",
    "    \\pi(a \\vert s, \\theta) = \\frac{e^{h(s, a; \\theta)}}{\\sum_{a'} e^{h(s, a'; \\theta)} }\n",
    "$$\n",
    "where $h(s, \\cdot ; \\theta)$ is the output of a neural network parameterized by $\\theta$ when given the state $s$ as input.\n",
    "\n",
    "In particular\n",
    "$$\n",
    "    \\log \\pi(a \\vert s, \\theta) = h(s, a; \\theta) - \\log \\bigg( \\sum_{a'} e^{h(s, a'; \\theta)} \\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693e15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05ef17a9",
   "metadata": {},
   "source": [
    "Question: Implement the get_action method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2866dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_SKELETON:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.episode_batch_size = episode_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "            QUESTION: \n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            #Your code here\n",
    "            pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        hidden_size = 128\n",
    "\n",
    "        obs_size = self.observation_space.shape[0]\n",
    "        n_actions = self.action_space.n\n",
    "\n",
    "        self.policy_net = Net(obs_size, hidden_size, n_actions)\n",
    "\n",
    "        self.scores = []\n",
    "        self.current_episode = []\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.policy_net.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "        self.n_eps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "529df899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_SKELETON:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.episode_batch_size = episode_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "            SOLUTION\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            unn_log_probs = self.policy_net.forward(state_tensor).numpy()[0]\n",
    "            p = np.exp(unn_log_probs - np.min(unn_log_probs))\n",
    "            p = p /  np.sum(p)\n",
    "            return np.random.choice(np.arange(self.action_space.n), p=p)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        hidden_size = 128\n",
    "\n",
    "        obs_size = self.observation_space.shape[0]\n",
    "        n_actions = self.action_space.n\n",
    "\n",
    "        self.policy_net = Net(obs_size, hidden_size, n_actions)\n",
    "\n",
    "        self.scores = []\n",
    "        self.current_episode = []\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.policy_net.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "        self.n_eps = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72743430",
   "metadata": {},
   "source": [
    "You should have an agent that runs but does not learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8810c67e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-2\u001b[39m\n\u001b[1;32m      7\u001b[0m agent \u001b[38;5;241m=\u001b[39m REINFORCE_SKELETON(action_space,\n\u001b[1;32m      8\u001b[0m         observation_space,\n\u001b[1;32m      9\u001b[0m         gamma,\n\u001b[1;32m     10\u001b[0m         episode_batch_size,\n\u001b[1;32m     11\u001b[0m         learning_rate,)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mrun_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 18\u001b[0m, in \u001b[0;36mrun_one_episode\u001b[0;34m(env, agent, display)\u001b[0m\n\u001b[1;32m     16\u001b[0m         clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m         plt\u001b[38;5;241m.\u001b[39mimshow(display_env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display:\n\u001b[1;32m     20\u001b[0m     display_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/pyplot.py:527\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/backend_bases.py:2164\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2164\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/figure.py:3154\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3154\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3158\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/axes/_base.py:3070\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3068\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3070\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3073\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/axis.py:1394\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;66;03m# Shift label away from axes to avoid overlapping ticklabels.\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_offset_text_position(tlb1, tlb2)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/axis.py:2368\u001b[0m, in \u001b[0;36mXAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;66;03m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;66;03m# that have been set by `fig.align_xlabels()`\u001b[39;00m\n\u001b[0;32m-> 2368\u001b[0m bboxes, bboxes2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick_boxes_siblings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2370\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mget_position()\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/axis.py:2161\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2159\u001b[0m axis \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_axis_map[name]\n\u001b[1;32m   2160\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 2161\u001b[0m tlb, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m bboxes\u001b[38;5;241m.\u001b[39mextend(tlb)\n\u001b[1;32m   2163\u001b[0m bboxes2\u001b[38;5;241m.\u001b[39mextend(tlb2)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/axis.py:1315\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1314\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1316\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1317\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1318\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/axis.py:1315\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1314\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1317\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1318\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/text.py:955\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get window extent of text w/o renderer. You likely \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwant to call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.draw_without_rendering()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m    956\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_layout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer)\n\u001b[1;32m    957\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/matplotlib/cbook.py:2044\u001b[0m, in \u001b[0;36m_setattr_cm\u001b[0;34m(obj, **kwargs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m             origs[attr] \u001b[38;5;241m=\u001b[39m sentinel\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2044\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2045\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(obj, attr, val)\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "gamma = 0.99\n",
    "episode_batch_size = 1\n",
    "learning_rate = 1e-2\n",
    "\n",
    "agent = REINFORCE_SKELETON(action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,)\n",
    "\n",
    "run_one_episode(env, agent, display=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "187e54e4",
   "metadata": {},
   "source": [
    "Time to implement the gradient updates. \n",
    "\n",
    "Remember you want to take \n",
    "$$\n",
    "    \\theta \\leftarrow \\theta + \\eta_t \\sum_{t=1}^T \\gamma^t G_t \\nabla_{\\theta} \\log \\pi(A_t \\vert S_t, \\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d254349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(REINFORCE_SKELETON):\n",
    "    \n",
    "    def _gradient_returns(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Turns a list of rewards into the list of returns * gamma**t\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        returns_list = []\n",
    "        T = len(rewards)\n",
    "        full_gamma = np.power(gamma, T)\n",
    "        for t in range(T):\n",
    "            G = rewards[T-t-1] + gamma * G\n",
    "            full_gamma /= gamma\n",
    "            returns_list.append(full_gamma * G)\n",
    "        return torch.tensor(returns_list[::-1])#, dtype=torch.float32)\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** Question **\n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated: \n",
    "            self.n_eps += 1\n",
    "            # Your code here\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520252bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(REINFORCE_SKELETON):\n",
    "    \n",
    "    def _gradient_returns(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Turns a list of rewards into the list of returns * gamma**t\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        returns_list = []\n",
    "        T = len(rewards)\n",
    "        full_gamma = np.power(gamma, T)\n",
    "        for t in range(T):\n",
    "            G = rewards[T-t-1] + gamma * G\n",
    "            full_gamma /= gamma\n",
    "            returns_list.append(full_gamma * G)\n",
    "        return torch.tensor(returns_list[::-1])#, dtype=torch.float32)\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** SOLUTION **\n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated: \n",
    "            self.n_eps += 1\n",
    "\n",
    "            states, actions, rewards = tuple(\n",
    "                [torch.cat(data) for data in zip(*self.current_episode)]\n",
    "            )\n",
    "\n",
    "            current_episode_returns = self._gradient_returns(rewards, self.gamma)\n",
    "\n",
    "            unn_log_probs = self.policy_net.forward(states)\n",
    "            log_probs = unn_log_probs - torch.log(torch.sum(torch.exp(unn_log_probs), dim=1)).unsqueeze(1)\n",
    "            score = torch.dot(log_probs.gather(1, actions).squeeze(), current_episode_returns).unsqueeze(0)\n",
    "            self.scores.append(score)\n",
    "            self.current_episode = []\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            full_neg_score = -score\n",
    "            full_neg_score.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c00e440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward before training =  -200.0\n",
      "episode = 50 , reward =  -200.0\n",
      "episode = 100 , reward =  -200.0\n",
      "episode = 150 , reward =  -200.0\n",
      "episode = 200 , reward =  -200.0\n",
      "episode = 250 , reward =  -200.0\n",
      "episode = 300 , reward =  -200.0\n",
      "episode = 350 , reward =  -200.0\n",
      "episode = 400 , reward =  -200.0\n",
      "episode = 450 , reward =  -200.0\n",
      "episode = 500 , reward =  -200.0\n",
      "episode = 550 , reward =  -200.0\n",
      "episode = 600 , reward =  -200.0\n",
      "episode = 650 , reward =  -200.0\n",
      "episode = 700 , reward =  -200.0\n",
      "episode = 750 , reward =  -200.0\n",
      "episode = 800 , reward =  -200.0\n",
      "episode = 850 , reward =  -200.0\n",
      "episode = 900 , reward =  -200.0\n",
      "episode = 950 , reward =  -200.0\n",
      "episode = 1000 , reward =  -200.0\n",
      "mean reward after training =  -200.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = .99\n",
    "episode_batch_size = 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "agent = REINFORCE(\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "        )\n",
    "N_episodes = 1000\n",
    "\n",
    "\n",
    "print(\"mean reward before training = \", np.mean(eval_agent(agent, env, 200)))\n",
    "# Run the training loop\n",
    "train(env, agent, N_episodes, eval_every=50,)\n",
    "\n",
    "# Evaluate the final policy\n",
    "print(\"mean reward after training = \", np.mean(eval_agent(agent, env, 200)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a99786b",
   "metadata": {},
   "source": [
    "In practice, one way to stabilize the updates is to wait a few episodes before doing the updates in batch. Implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBatch(REINFORCE):\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** QUESTION **\n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated:\n",
    "            self.n_eps += 1\n",
    "            # Your code here\n",
    "\n",
    "            if (self.n_eps % self.episode_batch_size)==0:\n",
    "               # Your code here\n",
    "               pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8fc207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBatch(REINFORCE):\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** SOLUTION **\n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated:\n",
    "            self.n_eps += 1\n",
    "\n",
    "            states, actions, rewards = tuple(\n",
    "                [torch.cat(data) for data in zip(*self.current_episode)]\n",
    "            )\n",
    "\n",
    "            current_episode_returns = self._gradient_returns(rewards, self.gamma)\n",
    "\n",
    "            unn_log_probs = self.policy_net.forward(states)\n",
    "            log_probs = unn_log_probs - torch.log(torch.sum(torch.exp(unn_log_probs), dim=1)).unsqueeze(1)\n",
    "            self.scores.append(torch.dot(log_probs.gather(1, actions).squeeze(), current_episode_returns).unsqueeze(0))\n",
    "            self.current_episode = []\n",
    "\n",
    "            if (self.n_eps % self.episode_batch_size)==0:\n",
    "                self.optimizer.zero_grad()\n",
    "                full_neg_score = - torch.cat(self.scores).sum() / self.episode_batch_size\n",
    "                full_neg_score.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16cb3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward before training =  20.605\n",
      "episode = 50 , reward =  18.6\n",
      "episode = 100 , reward =  29.2\n",
      "episode = 150 , reward =  37.5\n",
      "episode = 200 , reward =  43.0\n",
      "episode = 250 , reward =  41.3\n",
      "episode = 300 , reward =  50.0\n",
      "episode = 350 , reward =  66.7\n",
      "episode = 400 , reward =  61.7\n",
      "episode = 450 , reward =  45.1\n",
      "episode = 500 , reward =  36.7\n",
      "episode = 550 , reward =  40.1\n",
      "episode = 600 , reward =  61.0\n",
      "episode = 650 , reward =  63.4\n",
      "episode = 700 , reward =  68.3\n",
      "episode = 750 , reward =  86.3\n",
      "episode = 800 , reward =  96.5\n",
      "episode = 850 , reward =  83.8\n",
      "episode = 900 , reward =  84.1\n",
      "episode = 950 , reward =  83.4\n",
      "episode = 1000 , reward =  75.6\n",
      "episode = 1050 , reward =  69.2\n",
      "episode = 1100 , reward =  89.2\n",
      "episode = 1150 , reward =  70.4\n",
      "episode = 1200 , reward =  65.3\n",
      "episode = 1250 , reward =  66.6\n",
      "episode = 1300 , reward =  70.4\n",
      "episode = 1350 , reward =  97.6\n",
      "episode = 1400 , reward =  130.9\n",
      "episode = 1450 , reward =  207.1\n",
      "episode = 1500 , reward =  166.6\n",
      "episode = 1550 , reward =  117.2\n",
      "episode = 1600 , reward =  109.9\n",
      "episode = 1650 , reward =  121.2\n",
      "episode = 1700 , reward =  126.3\n",
      "episode = 1750 , reward =  134.9\n",
      "episode = 1800 , reward =  145.2\n",
      "episode = 1850 , reward =  159.8\n",
      "episode = 1900 , reward =  190.2\n",
      "episode = 1950 , reward =  219.7\n",
      "episode = 2000 , reward =  196.3\n",
      "episode = 2050 , reward =  192.5\n",
      "episode = 2100 , reward =  175.6\n",
      "episode = 2150 , reward =  171.2\n",
      "episode = 2200 , reward =  163.3\n",
      "episode = 2250 , reward =  229.5\n",
      "episode = 2300 , reward =  186.7\n",
      "episode = 2350 , reward =  170.5\n",
      "episode = 2400 , reward =  165.5\n",
      "episode = 2450 , reward =  142.1\n",
      "episode = 2500 , reward =  157.7\n",
      "episode = 2550 , reward =  173.6\n",
      "episode = 2600 , reward =  187.4\n",
      "episode = 2650 , reward =  156.7\n",
      "episode = 2700 , reward =  177.2\n",
      "episode = 2750 , reward =  157.7\n",
      "episode = 2800 , reward =  155.3\n",
      "episode = 2850 , reward =  185.6\n",
      "episode = 2900 , reward =  177.6\n",
      "episode = 2950 , reward =  155.4\n",
      "episode = 3000 , reward =  209.8\n",
      "mean reward after training =  180.84\n",
      "[255. 137. 145. 230. 109. 176. 198. 136. 298. 257.]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = .95\n",
    "episode_batch_size = 30\n",
    "learning_rate = 1e-2\n",
    "\n",
    "agent = REINFORCEBatch(\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "        )\n",
    "N_episodes = 3000\n",
    "\n",
    "\n",
    "print(\"mean reward before training = \", np.mean(eval_agent(agent, env, 200)))\n",
    "# Run the training loop\n",
    "train(env, agent, N_episodes, eval_every=50,)\n",
    "\n",
    "# Evaluate the final policy\n",
    "print(\"mean reward after training = \", np.mean(eval_agent(agent, env, 200)))\n",
    "print(eval_agent(agent, env, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c99315dd",
   "metadata": {},
   "source": [
    "Baseline:\n",
    "\n",
    "Remember we can add a baseline to reduce the variance. The simplest baseline one can take it use \n",
    "$$\n",
    "    b(S_t) \\equiv \\bar G_T = \\frac{1}{T} \\sum_{t=1}^T G_t\n",
    "$$\n",
    "which amount to subtracting the mean of the returns in the estimation to use \n",
    "\n",
    "$$\n",
    "        \\theta \\leftarrow \\theta + \\eta_t \\sum_{t=1}^T \\gamma^t (G_t - \\bar G_T) \\nabla_{\\theta} \\log \\pi(A_t \\vert S_t, \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b77dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBaseline(REINFORCE):\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** QUESTION **\n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated:\n",
    "            # Your code here\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32be0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBaseline(REINFORCE):\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** SOLUTION **\n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated:\n",
    "            self.n_eps += 1\n",
    "\n",
    "            states, actions, rewards = tuple(\n",
    "                [torch.cat(data) for data in zip(*self.current_episode)]\n",
    "            )\n",
    "\n",
    "            current_episode_returns = self._gradient_returns(rewards, self.gamma)\n",
    "            current_episode_returns = (current_episode_returns - current_episode_returns.mean())\n",
    "\n",
    "            unn_log_probs = self.policy_net.forward(states)\n",
    "            log_probs = unn_log_probs - torch.log(torch.sum(torch.exp(unn_log_probs), dim=1)).unsqueeze(1)\n",
    "            self.scores.append(torch.dot(log_probs.gather(1, actions).squeeze(), current_episode_returns).unsqueeze(0))\n",
    "            self.current_episode = []\n",
    "\n",
    "            if (self.n_eps % self.episode_batch_size)==0:\n",
    "                self.optimizer.zero_grad()\n",
    "                full_neg_score = - torch.cat(self.scores).sum() / self.episode_batch_size\n",
    "                full_neg_score.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34246f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward before training =  23.855\n",
      "episode = 50 , reward =  45.3\n",
      "episode = 100 , reward =  88.1\n",
      "episode = 150 , reward =  58.7\n",
      "episode = 200 , reward =  112.7\n",
      "episode = 250 , reward =  184.2\n",
      "episode = 300 , reward =  197.1\n",
      "mean reward after training =  212.185\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = .95\n",
    "episode_batch_size = 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "agent = REINFORCEBaseline(\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "        )\n",
    "N_episodes = 300\n",
    "\n",
    "\n",
    "print(\"mean reward before training = \", np.mean(eval_agent(agent, env, 200)))\n",
    "# Run the training loop\n",
    "train(env, agent, N_episodes, eval_every=50,)\n",
    "\n",
    "# Evaluate the final policy\n",
    "print(\"mean reward after training = \", np.mean(eval_agent(agent, env, 200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1226135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.episode_batch_size = episode_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** SOLUTION **\n",
    "        \"\"\"\n",
    "\n",
    "        self.current_episode.append((\n",
    "            torch.tensor(state).unsqueeze(0),\n",
    "            torch.tensor([[action]], dtype=torch.int64),\n",
    "            torch.tensor([reward]),\n",
    "        )\n",
    "        )\n",
    "\n",
    "        if terminated:\n",
    "            self.n_eps += 1\n",
    "\n",
    "            states, actions, rewards = tuple(\n",
    "                [torch.cat(data) for data in zip(*self.current_episode)]\n",
    "            )\n",
    "\n",
    "            current_episode_returns = self._gradient_returns(rewards, self.gamma)\n",
    "            current_episode_returns = (current_episode_returns - current_episode_returns.mean())\n",
    "\n",
    "            unn_log_probs = self.policy_net.forward(states)\n",
    "            log_probs = unn_log_probs - torch.log(torch.sum(torch.exp(unn_log_probs), dim=1)).unsqueeze(1)\n",
    "            self.scores.append(torch.dot(log_probs.gather(1, actions).squeeze(), current_episode_returns).unsqueeze(0))\n",
    "            self.current_episode = []\n",
    "\n",
    "            if (self.n_eps % self.episode_batch_size)==0:\n",
    "                self.optimizer.zero_grad()\n",
    "                full_neg_score = - torch.cat(self.scores).sum() / self.episode_batch_size\n",
    "                full_neg_score.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.scores = []\n",
    "\n",
    "        return  \n",
    "    \n",
    "    def _gradient_returns(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Turns a list of rewards into the list of returns * gamma**t\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        returns_list = []\n",
    "        T = len(rewards)\n",
    "        full_gamma = np.power(gamma, T)\n",
    "        for t in range(T):\n",
    "            G = rewards[T-t-1] + gamma * G\n",
    "            full_gamma /= gamma\n",
    "            returns_list.append(full_gamma * G)\n",
    "        return torch.tensor(returns_list[::-1])#, dtype=torch.float32)\n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "        Return action according to an epsilon-greedy exploration policy\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            unn_log_probs = self.policy_net.forward(state_tensor).numpy()[0]\n",
    "            p = np.exp(unn_log_probs - np.min(unn_log_probs))\n",
    "            p = p /  np.sum(p)\n",
    "            return np.random.choice(np.arange(self.action_space.n), p=p)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        hidden_size = 128\n",
    "\n",
    "        obs_size = self.observation_space.shape[0]\n",
    "        n_actions = self.action_space.n\n",
    "\n",
    "        self.policy_net = Net(obs_size, hidden_size, n_actions)\n",
    "\n",
    "        self.scores = []\n",
    "        self.current_episode = []\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.policy_net.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "        self.n_eps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "898bfde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards after training =  [500. 500. 500. 500. 500. 500. 500. 500. 500. 500.]\n",
      "Rewards Random =  [14. 17. 24. 16. 17. 15. 12. 25. 13. 19.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Rewards after training = \", eval_agent(agent, env, n_sim = 10))\n",
    "print(\"Rewards Random = \", eval_agent(RandomAgent(observation_space, action_space), env, n_sim = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e048805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some \n",
    "\n",
    "# a = np.eye(3)\n",
    "# for i in range(3): a[i, i] = i\n",
    "\n",
    "# unn_log_probs = torch.tensor(a)#.unsqueeze(0)\n",
    "# #print(unn_log_probs)\n",
    "# print(unn_log_probs.shape)\n",
    "# print(torch.log(torch.sum(torch.exp(unn_log_probs), dim=[1])).unsqueeze(1).shape)\n",
    "# log_probs = unn_log_probs - torch.log(torch.sum(torch.exp(unn_log_probs), dim=1)).unsqueeze(1)\n",
    "\n",
    "# print(unn_log_probs)\n",
    "# print(torch.log(torch.sum(torch.exp(unn_log_probs), dim=[1])).unsqueeze(1))\n",
    "# print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1680e663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode length 218.0\n"
     ]
    }
   ],
   "source": [
    "run_one_episode(env, agent, display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
